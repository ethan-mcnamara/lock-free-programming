# Preliminary Research on Lock-Free Programming

## Introduction
At the suggestion of Dr. Chester, three video lectures on the topic of lock-free programming were analyzed with the results summarized below. The purpose of the analysis was to determine the purpose of lock-free programming, common design patterns used in lock-free programming, the common pitfalls that arise from this programming style, and why this style of programming can often result in an onerous burden being placed on the developers. In the following week, an additional video (Video #4) was analyzed and the discussion of this video was merged into the previous discussion from the prior week.

## Video Lectures Analyzed
The three video lectures analyzed can be accessed through the following links. Throughout the below analysis, reference to these lectures will be done in the form of *Video #1/#2/#3/#4* rather than the titles of the lectures for reasons of conciseness and clarity. 
1. Introduction to Lock-free Programming - Tony van Eerd (Video #1)
  * [Video lecture link](https://www.youtube.com/watch?v=RWCadBJ6wTk "Introduction to Lock-free Programming - Tony van Eerd") 
2. Lock-Free Programming (or, Juggling Razor Blades) - Herb Sutter (Video #2)
  * [Video lecture link (Part I)](https://www.youtube.com/watch?v=c1gO9aB9nbs "Lock-Free Programming (or, Juggling Razor Blades) - Herb Sutter Part I")
  * [Video lecture link (Part II)](https://www.youtube.com/watch?v=CmxkPChOcvw "Lock-Free Programming (or, Juggling Razor Blades) - Herb Sutter Part II")
3. Live Lock-Free or Deadlock (Practical Lock-free Programming) - Fedor Pikus (Video #3)
  * [Video lecture link (Part I)](https://www.youtube.com/watch?v=lVBvHbJsg5Y "Live Lock-Free or Deadlock (Practical Lock-free Programming) - Fedor Pikus Part I")
  * [Video lecture link (Part II)](https://www.youtube.com/watch?v=1obZeHnAwz4 "Live Lock-Free or Deadlock (Practical Lock-free Programming) - Fedor Pikus Part II")
4. Nonblocking data structures - Michael Scott (Video #4)
  * [Video lecture link (Part I)](https://youtu.be/9XAx279s7gs "Michael Scott — Nonblocking data structures. Part 1.")
  * [Video lecture link (Part II)](https://youtu.be/cQIktrroRL0 "Michael Scott — Nonblocking data structures. Part 2.")

## Purpose of Lock-Free Programming
Lock-free programming is designed to improve the performance of multi-threaded memory-bound applications. In this case, a memory-bound application is one for which the largest bottlenecks exist in accessing data in memory. While lock-free programming can be applied to single-threaded applications, the first step in this approach would be to convert the single-threaded program into a multi-threaded program and then apply lock-free techniques so it is simpler to apply the definition solely to multi-threaded applications.

Lock-free techniques, like mutexes and locks, aim to eliminate race conditions from multi-threaded applications. Race conditions can occur when two or more threads are accessing a single variable simultaneously and the result of the program will depend on the essentially random nature of which thread completes its operations first. The traditional approach to race conditions has been to incorporate locks that lock a variable during the time in which a thread is accessing said variable and unlock it after the thread has completed its work. While this works well in a simple example, problems arrise when there are several locks in a program and two threads can each hold the lock the other desires but neither is willing to release the lock they hold, leading to a situation commonly known as a *deadlock*. Additionally, a traditionally common reason to avoid locks has been the fear of thread failure, i.e. a thread dies holding a lock, leading to a permanent deadlock as the now-dead thread can not release the lock. While in theory this could occur, Scott (*Video #4*) says this is not common in practise and a more valid reason against locks is OS preemption, which may result in the OS attempting to execute and instruction requiring the lock before the operation releasing the lock which would have negative performance consequences. Also, should a low-priority thread hold the lock desired by several high-priority threads, the low-priority thread may never get an oppourtunity to execute the release should a sufficient number of high-priority threads always be present in the queue.

Additionally, as van Eerd speaks to in *Video #1*, there can exist race conditions in places it would not seem possible for a race condition to exist, such as between checking a variable in an `if` condition and writing to said variable on the next line. These race conditions are due to the reordering and scheduling done by the operating system of the CPU, which again can lead to undesired results.

Lock-free programming, true to its name, typically does not use locks in its implementations. Instead, atomic compare-and-swap (CAS) operations are used, often inside `while` loops, to determine whether a value of a variable read at an earlier time is still equal to the variable's current value before writing to said variable. The common implementation will involve a `do-while` loop where inside to the `do` statement the value is read and stored in a temporary variable before writing to another temporary variable the desired new value of the variable. Inside the `while` statement a CAS function is called that checks whether the value read and stored is still equal to the variable in memory in which case the new desired value is written and breaks from the loop and if not the loop reiterates.

This lock-free approach is superior to the mutex approach in that it guarantees that at least one thread will at all time be making progress, i.e. there exists no possibility of a deadlock where no threads are making any progress. While there are some other considerations that need to be made, that is the common rationale for choosing a lock-free approach compared to an approach using locks.

## Common Design Patterns and Techniques in Lock-Free Programming
As mentioned in the above sections, and indeed in all three video lectures, the main design technique of lock-free programming is to use a CAS function call. It is important to remember however, that the CAS function must be atomic in its implementation or the validity of the program no longer holds. Because the CAS function is essentially two operations, the first being a conditional to check whether the two values are equal with the second being a write operation, if these are not atomic, this is equal to the example mentioned in *Video #1* where an `if` condition is written on the line directly above a write instruction. Without atomicity, it is impossible to say definitively whether or not the value read has been updated at the time of writing.

Also, due to the nature of the CAS function van Eerd explains in *Video #1* that should manipulation of multiple variables be required in an atomic nature, the CAS function is not the ideal tool due to the fact that it is only considering the value of a single variable at a time and cannot support the examination of multiple variables at once. Should the atomic manipulation of multiple variables be required, each of Sutter (*Video #2*) and Pikus (*Video #3*) had suggestions for maintaining the use of the CAS function without sacrificing the program's validity. Sutter's sugestion, which is likely the one most developers will first consider, is to wrap the variables within a `struct` or `class` and manipulate the abstracted data structure rather than the primitive data structures. Sutter warned that while this approach does work well for objects of smaller size, he explained that when these objects are sufficiently large, locks are used by the CPU when accessing this data as they may require multiple cache reads to fully bring into main memory which may cause unintended performance degredation. Pikus's solution, which is arguably more complex and narrow in its use cases, is to pack smaller variables into a single 64-bit word which can easily be manipulated atomically. This approach only applies when multiple variables smaller than 64-bits are being manipulated and will require custom logic for manipulating the variables but may be the ideal approach for some programs. 

While the first three videos spoke primarely to the atomic CAS operation, there does exist a second set of atomic operations which can be used to perform a similar task to CAS. These atomic operations, which are frequently used in conjunction, are the `load_linked(...)` (LL) and the `store_conditional(...)` (SC) instructions. The LL instructions atomically loads a word into the L1 cache and tags the cache line on which the data was loaded. This tag represents that this word has not been updated by this thread or any other thread. Should another thread load the same word and modify it after the first thread tagged the word in their cache line, the cache coherency protocol of the CPU would remove the cache line tag. The SC writes a modified version of the word back to the cache line only if the line is still tagged. If the cache line is no longer tagged, the SC operation returns false. By calling the LL operation, modifying the shared variable and storing the result in a temporary variable and then calling the SC operation to write the desired value only if the cache line is still tagged, non-blocking writing of a variable can still occur. 

An issue experienced only by the LL & SC approach is that cache lines may be untagged for reasons other than the expected data not being present. It is possible that through the background OS processes the cache line becomes untagged, causing an additional iteration of the loop used with this approach.

The LL & SC approach can be superior to the CAS approach as the ABA problem is abated. The ABA problem refers to the reading of an object by thread A at one point, the deletion of the same object by thread B immediately after, and then the creation of a new object by thread B which is assigned the same memory location as the just-deleted object by the OS, all before thread A writes to the object in memory. From the perspective of thread A, the object read initially and the object read at the time of writing in the CAS function are identical as they are both the same object type in the same memory address, despite the fact their contents might be very different. The LL & SC approach will not experience this issue as once thread B modifies or deleted the object, the cache line of thread A will no longer be tagged, at which point the SC operation is guaranteed to fail, causing another LL operation to be required.

It is possible to use CAS and avoid the ABA issue, but additional data is required in the pointers to ensure they are not inappropriately reused. In *Video #4*, Scott suggested an approache using counted pointers. Counted pointers add a serial number to each pointer class. This serial number is of sufficient size so it is very unlikely that rollover will occur such that two pointers are declared to the same memory address, with the same serial number, all before the first thread has read the address again after the initial read. Since the address and serial number must now match the initial read, it is sufficiently probable that the ABA issue will not occur. A downside of this approach is the additional comparison that must now be done to atomically compare both the pointer and the serial number. Unless the serial number can be especially small, it is likely a double-word CAS operation must be used, which limits the machines on which this program could run as not all machine support a double-word CAS.

Another example of the ABA problem which was not shown in any of the videos is the allocation, deletion, and subsequent re-allocation of a leaf node in a tree. Should the value stored in this node be wished to be modified by thread A, it will first read the address and value into temporary variables before modifying the temporary values. At the same time, it is possible that thread B also wishes to modify this node but chooses to delete the node instead. Thread B then creates a new node with the same value but places it in a different location in the tree. By coincidence, the node created by thread B is assigned the same memory location as the node previously deleted by thread B. So the node referenced by thread A no longer exists, but a node with the same value in the same memory adress does exist, despite being a leaf node in a different part of the tree. Thread A awakes and wishes to perform its CAS operation to modify the node. Because it is looking at the node in memory, and since this implementation of a tree only stores the node's value and its children (which will be null because it is a lead node), thread A believes the node it initially read has not been modified in the interim and will then continue with its write operation despite the operations of thread B which have rendered the operations of thread A invalid. Without a mechanism to moderate this issue, thread A will continue with the write and the program logic may suffer unintented consequences.

## Proving Lock-Free Algorithms are Correct
In *Video #4*, Scott speaks to the requirements of a correct algorithm. According to him, for an algorithm to be correct, it must have safety and liveness. He defines safety as the guarantee that "bad things" will not happen and defines liveness as the guarantee that "good things" will eventually happen. Individually, these requirements can be trivially satisfied but together prove an algorithm to be correct. 

A technique to prove the correctness of an algorithm is linearizability. The execution history of an algorithm (the list of every operation an algorithm executes which for a multi-threaded algorithm may not be a sequential list) is linearizable if it is equivalent (containing the same invocation and return operations with the same argument values) to a sequential execution S that respects: (a) object semantics, and (b) "real-time" order. Object semantics refers to the fact that the requirements of the call are met, i.e. a `pop` operation will always return the top of a stack. "Real-time" order is the property that the return values of operations will reflect the true order in which they were called, i.e. two subsequent `pop` operations will properly return the intial head value to the first call and the next head value to the second call.

An equivalent explanation of the linearizability property is that every operation must appear to happen instantaneously at a single moment in time. For simpler algorithms, this may be a line of code in which an atomic operation was invoked. For more complex operations, this may not be so trivial. This point in the execution is known as the linearization point. The linearization point is frequently used when proving an algorithm's linearizability. 

Scott also mentioned the composability property of linearizability. This property implies that if structure A is linearizable and structure B is linearizable, then all histories using both structure A and structure B are linearizable. 

A common mistake in proving an algorithm's linearizability is believing an operation's linearization point is after it completes its required task. Often, the linearization point is located after an operation has completed a portion of its task but still has additional work to do. Scott called this additional work "clean-up work" as it could be done by any thread, not necessarily the thread who performed the work preceding and including the linearization point. Since the linearization point is the point at which the task has instantaneously occured, the algorithm could be designed that between that point and the true end of the task, other threads may recognize that the task is not fully complete and complete the task. Therefore when proving the linearizability of an algorithm, it is important to recognize where the dividing line exists between the work including and preceding the linearization point and the "clean-up work". Incorrectly recognizing this point could render an incorrect or invalid proof of linearizability. 

## Lock-Free Data Structure Example
In *Video #4*, Scott presented many lock-free data structures but one which appeared to act as the foundation for other lock-free data structures was the Treiber stack, which was first presented by R. Kent Reiber in his 1986 article ["Systems Programming: Coping with Parallelism"](https://dominoweb.draco.res.ibm.com/58319a2ed2b1078985257003004617ef.html "Systems Programming: Coping with Parallelism"). The Treiber stack is a scalable lock-free stack which uses CAS operations to perform its `push` and `pop` operations. The psuedocode below is taken from the Scott's slides to demonstrate how the Treiber stack is implemented.

```
class stack
  <node*, int> top

void stack.push(node* n):
  repeat
    <o, c> := top
    n->next := o
  until CAS(&top, <o, n>, <n, c>)

node* stack.pop():
  repeat
    <o, c> := top
    if o = null return null
    n := o->next
  until CAS(&top, <o, c>, <n, c+1>)
  return o
```

While it is known that this data structure is lock-free, it is not necessarily true that this algorithm is correct. To prove the correctness of this algorithm, the linearization points of each of the functions must be determined. For the `push` operation, the linearization point is the line on which the `CAS` function is called. Due to the atomic nature of the `CAS` function, the element to be added to the stack is added exactly at the successful calling of the `CAS` function. Should that function fail, the loop will repeat at which point the previous calling of the `CAS` function is no longer the operation's linearization point. For the `pop` operation, the linearization point will depend on whether there exist elements in the stack or not. If the stack is empty when the `pop` operation is called, the linearization point is the `if o = null return null` line. This is because at this point the `pop` operation has completed its work and can be said to have executed at that exact moment. If the stack is not empty, the linearization point is the first successful calling of the `CAS` function, like the `push` function as this is the moment at which the shared data structure is modified and the operation has completed its task. Unlike the `push` function, the `pop` function does have additional instructions after its linearization point, namely the returning of the element popped. According to Scott, this work is known as "clean-up work" and could theoretically be done by any thread who encounters this unfinished work, although with an example this trivial it is unlikely that would occur. 

Because the linearization point of every operation utilized in this algorithm has been identified, it can be said that this algorithm is correct.

## Universal Constructors
Scott brought up the concept of universal constructors near the end of *Video #4*. Universal constrcutors are constructors which can take a correct sequential algorithm and produce a correct non-blocking concurrent code. These constructors have two main purposes. The first is as a proof of concept to show that this sequential algorithm can indeed be converted into an equally-correct concurrent program. The latter purpose is to develop an algorithm which may actually be used. According to Scott, this second use is much less common as the code produced by universal constructors is often not as optimized as algorithms developed manually by developers and it is rare for the code produced by universal constructors to be of significant value to a library or product team. Universal constructors have been used to show that every non-blocking data structure can be reduced to an algorithm whose only hardware primitives are single-word `CAS` or `load_linked` and `store_conditional` operations. Additionally, universal constructors can be used to convert a lock-free program into a fast wait-free algorithm with a high space overhead. To do so each thread evaluates itself and determines whether it has existed for a sufficient period of time and whether it considers itself to be starving. If so, it announces on a shared platform that it is starving and is then allowed to proceed ahead of other threads who may not be starving. This shared message platform is the source of the higher space overhead but does remove the potential for a thread to starve permenantly, which does exist in lock-free programming.

## Why is Lock-Free Programming so Hard and is it Worth the Hastle?
Lock-free programming is an optimization technique and as a result, is not a style of programming with which many developers are familiar. If the goal of the developer is to develop an application in a reasonable amount of time, very few would willingly incorporate lock-free techniques from the outset as this will place an undue burden on the development of the application. As a result, while lock-free programming is a technique that can be difficult to incorporate, the main reason it is viewed with such trepidation is a lack of a solid understanding of the technique's benefits and implementation details.

That being said, even if every software engineering new grad had been provided an in-depth look into lock-free programming, it is unlikely most new applications would incorporate this programming technique due to the technical difficulties that can result from developing and debugging lock-free applications. As Pikus mentioned in *Video #3*, there are a number of considerations that must be made when considering implementing lock-free techniques into an existing program. Firstly, poor application performance should be the primary reason for implementing lock-free programming. Some applications run on hardware or operate in environments where performance is less of a factor and as a result the development cost to create a lock-free implementation will not net a positive return on investment. Assuming performance is the driving factor, lock-free programming is typically only the ideal approach when the number of shared variables is minimal. As Pikus explained, with a large volume of shared variables, the complexity of lock-free algorithms increases significantly, potentially to a level where the performance gains achieved by the lock-free implementation are outweighed by the strain the data synchronization costs are placing on the program. Overall, if the conditions are met such that a lock-free approach is the ideal solution and a development team exists such that this change will not place an overly burdensome task on their plates, lock-free programming is likely to reap the rewards it promises and be worth the hassle of development and debugging such a program.

The answer for first half of the question acting as a subtitle for this section can be determined by examining the common problems developers experience with multi-threaded applications, lock-free or otherwise. Multi-threaded applications are notoriously difficult to debug and test due to the sometimes unpredictable nature of their instructions. The situation can also occur where a bug does not exist in development or testing but then appears suddenly once deployed to production due a singular race condition having not yet occurred to that point. The common approach to these issues is to use mutexes or locks, but of course these bring their own problems that require management. Lock-free programming takes these impediments and incorporates more complex methods of dealing with them than simple locks that are acquired before a critical section and released afterwards. Instead, atomic instructions with which most developers have less familiarity are used. This approach to solving multi-threaded programs is not necessarily more difficult than using locks, but the lack of familiarity with the tools available, combined with the already-present difficulties of multi-threaded programming breeds a programming style that is commonly feared despite the benefits of the technique and its relative simplicity when examined deeper. 

## Conclusion
Lock-free programming is a technique that can provide a significant benefit to an application's performance, provided it is implemented properly and the proper considerations are made to ensure that lock-free programming is indeed the ideal optimization technique for the problem at hand. While it does avoid many of the pitfalls incurred by other multi-threaded programming styles, it does come with its own that must be handled accordingly to guarantee the application under development is running efficiently and properly.
